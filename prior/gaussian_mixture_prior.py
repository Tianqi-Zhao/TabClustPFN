"""
Gaussian mixture prior class for synthetic dataset generation.

This module contains the GaussianMixturePrior class that generates
synthetic datasets using Gaussian mixture models with parameters
generated by the MixSim R package.
"""

from __future__ import annotations

import numpy as np
import torch
from torch import Tensor
from typing import Dict, Tuple, Optional, Any
import logging

from .base import StructuredPrior
from .hp_sampling import HpSamplerList
from .gaussian_mixture import GaussianMixture
from configs.prior_config import DEFAULT_FIXED_HP, DEFAULT_SAMPLED_HP
from .utils import generate_mixsim_params, resample_weights_dirichlet  # Import utility functions

logger = logging.getLogger(__name__)

class GaussianMixturePrior(StructuredPrior):
    """Generate features X and labels y from Gaussian mixture distribution, using similar structure as SCMPrior"""

    def __init__(
        self,
        batch_size: int = 256,
        batch_size_per_gp: int = 4,
        batch_size_per_subgp: Optional[int] = None,
        min_features: int = 2,
        max_features: int = 100,
        max_classes: int = 10,
        min_seq_len: Optional[int] = None,
        max_seq_len: int = 1024,
        log_seq_len: bool = False,
        seq_len_per_gp: bool = False,
        replay_small: bool = False,
        fixed_hp: Dict[str, Any] = DEFAULT_FIXED_HP,
        sampled_hp: Dict[str, Any] = DEFAULT_SAMPLED_HP,
        n_jobs: int = -1,
        num_threads_per_generate: int = 1,
        device: str = "cpu",
    ):
        super().__init__(
            batch_size=batch_size,
            min_features=min_features,
            max_features=max_features,
            max_classes=max_classes,
            min_seq_len=min_seq_len,
            max_seq_len=max_seq_len,
            log_seq_len=log_seq_len,
            replay_small=replay_small,
            n_jobs=n_jobs,
            num_threads_per_generate=num_threads_per_generate,
            device=device,
        )
        self.batch_size_per_gp = batch_size_per_gp
        self.batch_size_per_subgp = batch_size_per_subgp or batch_size_per_gp
        self.seq_len_per_gp = seq_len_per_gp
        # Delay MixSim import to avoid pickle issues with multiprocessing
        # self.mixsim_r = importr('MixSim')
        self._fixed_hp = fixed_hp
        self._sampled_hp = sampled_hp

    @property
    def fixed_hp(self) -> Dict[str, Any]:
        return {
            "PiLow": self._fixed_hp["PiLow"],
            "int": self._fixed_hp["int"],
            "ecc": self._fixed_hp["ecc"],
            "dirichlet_alpha_binary": self._fixed_hp["dirichlet_alpha_binary"],
            "dirichlet_alpha_multiclass": self._fixed_hp["dirichlet_alpha_multiclass"],
            "resample_weights": self._fixed_hp.get("resample_weights", True),  # Default True for backward compatibility
        }

    @property
    def sampled_hp(self) -> Dict[str, Any]:
        return {
            "MaxOmega": self._sampled_hp["MaxOmega"],
            "BarOmega": self._sampled_hp["BarOmega"],
            "hom": self._sampled_hp["hom"],
            "sph": self._sampled_hp["sph"],
        }

    def hp_sampling(self, generator: Optional[torch.Generator] = None) -> Dict[str, Any]:
        """
        Sample hyperparameters for Gaussian mixture dataset generation.

        Returns
        -------
        dict
            Dictionary with sampled hyperparameters
        """
        hp_sampler = HpSamplerList(self.sampled_hp, device=self.device, generator=generator)
        return hp_sampler.sample()

    @torch.no_grad()
    def generate_dataset(self, params: Dict[str, Any]) -> Tuple[Tensor, Tensor, Tensor]:
        """
        Generates a single dataset using Gaussian mixture model.

        Parameters
        ----------
        params : dict
            Hyperparameters for generating this dataset

        Returns
        -------
        tuple
            (X, y, d) where X is features, y is labels, d is num_features
        """
        
        generator = params["generator"]
        attempt_count = 0
        
        while True:
            attempt_count += 1
            
            try:
                # Use the utility function to get GM parameters
                # generate_mixsim_params will retry 10 times internally with different random seeds
                # If it fails, raises RuntimeError which is caught by outer loop to continue retrying
                mixsim_weights, means, covariances = generate_mixsim_params(
                    num_classes=params["num_classes"],
                    num_features=params["num_features"],
                    hom=params["hom"],
                    sph=params["sph"],
                    PiLow=params["PiLow"],
                    int_vec=params["int"],
                    MaxOmega=params["MaxOmega"],
                    ecc=params.get("ecc", 0.9),
                    device=self.device,
                    generator=generator,
                    resN=50,  # MixSim's internal retry parameter
                    max_retries=5,  # Outer retries with seed changes
                )
                
                # Decide whether to use MixSim's original weights or resample using Dirichlet distribution
                if params.get("resample_weights", True):
                    # Resample weights using Dirichlet distribution for controlled class balance
                    # MixSim weights are replaced to ensure consistent balance behavior with GMIResPrior
                    weights = resample_weights_dirichlet(
                        num_classes=params["num_classes"],
                        generator=generator,
                        alpha_binary=params["dirichlet_alpha_binary"],
                        alpha_multiclass=params["dirichlet_alpha_multiclass"],
                        device=self.device,
                    )
                else:
                    # Use MixSim's original weights
                    weights = mixsim_weights
                
                # Create GaussianMixture instance with params
                gm = GaussianMixture(
                    seq_len=params["seq_len"],
                    num_features=params["num_features"],
                    num_classes=params["num_classes"],
                    weights=weights,
                    means=means,
                    covariances=covariances,
                    device=self.device,
                    generator=generator,
                )

                # Generate data
                X, y = gm()

                # Pad X to max_features
                if params["num_features"] < params["max_features"]:
                    padding = torch.zeros(params["seq_len"], params["max_features"] - params["num_features"], device=self.device)
                    X = torch.cat([X, padding], dim=1)

                # Add batch dim for compatibility
                X, y = X.unsqueeze(0), y.unsqueeze(0)
                d = torch.tensor([params["num_features"]], device=self.device, dtype=torch.long)

                # For Gaussian mixture, features are unlikely to be constant, so skip filtering
                return X.squeeze(0), y.squeeze(0), d.squeeze(0)
            
            except Exception as e:
                # Catch all exceptions (including MixSim timeout/failure) and log for visibility in distributed runs
                logger.warning(
                    "GaussianMixturePrior dataset generation failed (attempt %d): %s | params={num_classes:%s, num_features:%s, seq_len:%s, MaxOmega:%s}",
                    attempt_count,
                    e,
                    params.get("num_classes"),
                    params.get("num_features"),
                    params.get("seq_len"),
                    params.get("MaxOmega"),
                )
                
                # Advance generator to ensure next retry uses different random seed
                # This ensures generate_mixsim_params uses different random seeds internally
                _ = torch.empty((), dtype=torch.int64, device='cpu').random_(generator=generator).item()
                
                # Prevent infinite loop by setting retry limit
                # Note: Should be a small number (3-5) because params are fixed
                # If this fails, outer layer (get_batch) should resample params
                if attempt_count >= 5:
                    raise RuntimeError(
                        f"Failed to generate a valid dataset after {attempt_count} attempts. Last error: {e}"
                    ) from e
                
                continue

    def get_prior(self, generator: Optional[torch.Generator] = None) -> str:
        return "gm"